{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859583e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tabula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b864cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для очистки сырой таблицы\n",
    "\n",
    "def clean(data):\n",
    "    \n",
    "    # Очистка таблицы от лишних значений в столбце с регионами, удаление полностью пустых столбцов    \n",
    "    clean_data = data[data[0].str.lower() != 'республика, край, область']\n",
    "    clean_data = clean_data[~clean_data[0].str.startswith('*')].dropna(subset=[0]).dropna(axis=1, how='all')\n",
    "\n",
    "    # Если регионы склеились с первым столбцом значений, разбиваем их\n",
    "    if clean_data[0].str.contains(r'\\d').any():\n",
    "        region = clean_data[0].str.replace(r'\\d+', '', regex=True).str.rstrip() \n",
    "        missed_data = clean_data[0].str.extractall(r'(\\d+)')\n",
    "        missed_data = missed_data.groupby(level=0)[0].apply(lambda x: ''.join(x))\n",
    "        clean_data = clean_data.drop(columns=0)\n",
    "        clean_data.insert(0, '0', region)\n",
    "        clean_data.insert(1, 'missed_data', missed_data)    \n",
    "        \n",
    "    #Разбиваем столбцы со значениями, которые не должны были склеиваться - по дефисам или по пробелам  \n",
    "    for col in clean_data.columns[1:]:\n",
    "        if (clean_data[col].str.contains('--', na=False).any()) and (~clean_data[col].str.contains(r'\\d', na=False).any()):\n",
    "            split_data = clean_data[col].str.split('-', expand=True).drop(columns = 0).replace('', '-')\n",
    "            split_data.columns = range(split_data.shape[1])\n",
    "        else:\n",
    "            split_data = clean_data[col].str.split(r'\\s+', expand=True)\n",
    "        for new_col in range(split_data.shape[1]):\n",
    "            clean_data[f'{col}_{new_col}'] = split_data[new_col]\n",
    "        clean_data.drop(columns = col, inplace=True)\n",
    "        \n",
    "   # Склеиваем столбцы, которые в источнике были одним\n",
    "    def combine_columns(data):\n",
    "        cols = data.columns[1:]\n",
    "        i = 0\n",
    "        while i < len(cols) - 1:\n",
    "            if (data[cols[i]].isnull() ^ data[cols[i+1]].isnull()).all():\n",
    "                data[cols[i]] = data[cols[i]].combine_first(data[cols[i+1]])\n",
    "                data.drop(columns=cols[i+1], inplace=True)\n",
    "                cols = data.columns\n",
    "            else:\n",
    "                i += 1\n",
    "        return data\n",
    "    clean_data = combine_columns(clean_data)\n",
    "    clean_data.columns = range(clean_data.shape[1])\n",
    "    return clean_data\n",
    "\n",
    "# Функция для обработки названий регионов\n",
    "def only_lower_letters(column):\n",
    "    def inner_func(s):\n",
    "        return ''.join([str.lower(char) for char in s if char.isalpha()])  \n",
    "    return [inner_func(s) for s in column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b282bb8",
   "metadata": {},
   "source": [
    "1. Подгружаем справочники (indicators, nosologies, years, regions) и приводим их в удобный вид."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071b5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = pd.read_csv('data/raw/appendix/indicators.csv', encoding='windows-1251', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068e4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "nosologies = pd.read_csv('data/raw/appendix/nosologies.csv', encoding='windows-1251', sep=';')\n",
    "nosologies = pd.melt(nosologies, \\\n",
    "                     id_vars=nosologies.columns[:5], \\\n",
    "                     value_vars=nosologies.columns[5:], \\\n",
    "                     var_name='year', value_name='table_number', ignore_index=False)\n",
    "nosologies = nosologies.dropna(subset=['table_number']) \\\n",
    "                    .astype({'year': 'int', 'table_number': 'int'}) \\\n",
    "                    .sort_values(['year', 'table_number'])\n",
    "                        \n",
    "# Создаем столбец, показывающий номер первой части таблицы по порядку расположения таблиц внутри сборника\n",
    "table_order = []\n",
    "for y in nosologies.year.unique():\n",
    "    x=0\n",
    "    for p_q in nosologies.query('year == @y').pages_quantity:\n",
    "        table_order.append(x)\n",
    "        x += p_q\n",
    "nosologies['table_order'] = table_order\n",
    "\n",
    "# Обрабатываем комментарии, чтобы позже подтянуть их только к определенным индикаторам\n",
    "nosologies = nosologies.merge(nosologies.comment.str.split(';indicator_short_code = ', expand=True), \\\n",
    "                                left_index=True, right_index=True) \\\n",
    "                        .drop(columns='comment') \\\n",
    "                        .drop_duplicates() \\\n",
    "                        .rename(columns={0: 'comment', 1: 'comment_for_indicator'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe468f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = pd.read_csv('data/raw/appendix/years.csv', encoding='windows-1251', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71b0382",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = pd.read_csv('https://raw.githubusercontent.com/tochno-st/regions_dict/main/regions_etalon_v2.0.csv', \\\n",
    "                      sep=';', dtype='string')\n",
    "regions = regions[['name_rus', 'okato', 'oktmo', 'level']]\n",
    "\n",
    "regions['region'] = only_lower_letters(regions.name_rus)\n",
    "regions = regions.rename(columns = {'name_rus': 'object_name', \\\n",
    "                                    'okato': 'object_okato', \\\n",
    "                                    'oktmo': 'object_oktmo', \\\n",
    "                                    'level': 'object_level'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0459a",
   "metadata": {},
   "source": [
    "2. Обрабатываем сборники."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e40fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем пути сборников\n",
    "folder = \"data/raw/digests/\"\n",
    "paths = [folder + i for i in os.listdir(folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d49357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем датафрейм для данных из всех сборников\n",
    "all_data_pdf = pd.DataFrame()\n",
    "\n",
    "# Для каждого сборника из папки:\n",
    "for file_path in paths: \n",
    "    \n",
    "    # 1. Достать год из названия файла.\n",
    "    year_pdf = int(file_path[-8:-4])\n",
    "    \n",
    "    # 2. В соответствии с годом достать страницы из справочника years.\n",
    "    pages_pdf = years.query('year == @year_pdf').pages.iloc[0] \n",
    "    \n",
    "    # 3. Считать необходимые страницы из pdf.\n",
    "    raw_data_pdf = tabula.read_pdf(file_path, pages=pages_pdf, stream=True, area=[162,0,813,595], \\\n",
    "                                   pandas_options={'header': None, 'dtype': 'string'})\n",
    "    \n",
    "    # 4. Собрать список номеров первых частей таблиц.\n",
    "    first_tables = nosologies.query('year == @year_pdf').table_order.to_list()\n",
    "   \n",
    "    # 5. Создать датафрейм для сборника.\n",
    "    year_data_pdf = pd.DataFrame()\n",
    "    \n",
    "    # 6. Обработать считанные таблицы в сборнике.\n",
    "    # Для каждой первой части таблицы:\n",
    "    for i in first_tables:\n",
    "        \n",
    "        # - прогнать ее через функцию clean\n",
    "        # - соединить ее с частями на следующих страницах (всего бывает 2 части или 4 - см. pages_quantity)\n",
    "        table = pd.concat([clean(raw_data_pdf[i]), clean(raw_data_pdf[i+1])], ignore_index=True)\n",
    "        if nosologies.query('year == @year_pdf & table_order == @i').pages_quantity.iloc[0] == 4:\n",
    "            table = table.merge(pd.concat([clean(raw_data_pdf[i+2]), clean(raw_data_pdf[i+3])], ignore_index=True), \\\n",
    "                                how='left', on=0)\n",
    "        # - проставить соответствующие коды колонок-индикаторов\n",
    "        table.columns = range(table.shape[1])\n",
    "        table_data_pdf = pd.DataFrame(table)\n",
    "        col = years.query('year == @year_pdf').indicators_order.iloc[0].split(';')\n",
    "        col.insert(0, 'region')\n",
    "        table_data_pdf.columns = col[:table.shape[1]]\n",
    "        \n",
    "        # - сделать анпивот таблицы\n",
    "        table_data_pdf = pd.melt(table_data_pdf, id_vars=table.columns[0], value_vars=table.columns[1:], \\\n",
    "                                    var_name='indicator_short_code', value_name='indicator_value', ignore_index=False)\n",
    "        \n",
    "        # - добавить столбцы с соответствующими данными из приложения nosologies\n",
    "        nosologies_data = nosologies.query('year == @year_pdf & table_order == @i') \\\n",
    "                                [['nosology_name','nosology_code','soc_dem_group', 'comment', 'comment_for_indicator']].iloc[0]\n",
    "        table_data_pdf[['nosology_name','nosology_code','soc_dem_group','comment','comment_for_indicator']] = nosologies_data\n",
    "        table_data_pdf.comment = [comment if indicator == comment_for_indicator else None\n",
    "                for indicator, comment, comment_for_indicator in \n",
    "                zip(table_data_pdf['indicator_short_code'], table_data_pdf['comment'], table_data_pdf['comment_for_indicator'])]\n",
    "        table_data_pdf = table_data_pdf.drop(columns='comment_for_indicator')\n",
    "        \n",
    "        # - добавить полученную таблицу в общий датафрейм для сборника\n",
    "        year_data_pdf = pd.concat([year_data_pdf, table_data_pdf], ignore_index=True)\n",
    "        \n",
    "    # Всем таблицам проставить столбец с годом текущего сборника.\n",
    "    year_data_pdf[['year']] = year_pdf\n",
    "    \n",
    "    # Добавить данные по сборнику в общий датафрейм со всеми сборниками\n",
    "    all_data_pdf = pd.concat([all_data_pdf, year_data_pdf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37383e7e",
   "metadata": {},
   "source": [
    "3. Приводим данные в нужный формат и дополняем данными из справочников."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca664333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменяем пропуски в значениях индикаторов, округляем значения до 4 знаков после запятой\n",
    "all_data_pdf.indicator_value = all_data_pdf.indicator_value.str.replace(',', '.')\n",
    "\n",
    "mask = (all_data_pdf.indicator_value.isnull()) & (all_data_pdf.year == 2011)\n",
    "all_data_pdf.loc[mask, 'indicator_value'] = '66666666'\n",
    "\n",
    "all_data_pdf.indicator_value = pd.to_numeric(all_data_pdf.indicator_value, errors='coerce').fillna(88888888).round(4)\n",
    "\n",
    "all_data_pdf.indicator_short_code = all_data_pdf.indicator_short_code.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ea8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приводим регионы в соответствие со справочником\n",
    "all_data_pdf.region = all_data_pdf.region.str.replace('г. ', '', regex=False)\n",
    "all_data_pdf.region = only_lower_letters(all_data_pdf.region)\n",
    "all_data_pdf = all_data_pdf.query(\"region != 'крымскийфо'\")\n",
    "replacements = {\n",
    "    'россия': 'российскаяфедерация',\n",
    "    'гормосква': 'москва',\n",
    "    'горсанктпетербург': 'санктпетербург',\n",
    "    'городcевастополь': 'севастополь',\n",
    "    'ингушскаяреспублика': 'республикаингушетия',\n",
    "    'республикасевосетия': 'республикасевернаяосетияалания',\n",
    "    'респсевернаяосетия': 'республикасевернаяосетияалания',\n",
    "    'кабардинобалкарскаяресп': 'кабардинобалкарскаяреспублика',\n",
    "    'респкабардинобалкария': 'кабардинобалкарскаяреспублика',\n",
    "    'карачаевочеркесскаяресп': 'карачаевочеркесскаяреспублика',\n",
    "    'респкарачаевочеркесия': 'карачаевочеркесскаяреспублика',\n",
    "    'пермскаяобласть': 'пермскийкрай',\n",
    "    'камчатскаяобласть': 'камчатскийкрай',\n",
    "    'республикачечня': 'чеченскаяреспублика',\n",
    "    'республикаудмуртия': 'удмуртскаяреспублика',\n",
    "    'республикачувашия': 'чувашскаяреспублика',\n",
    "    'архангельскаяоблбао': 'архангельскаяобластьбезавтономногоокруга',\n",
    "    'тюменскаяоблбао': 'тюменскаяобластьбезавтономныхокругов',\n",
    "    'тюменскаяоблбезао': 'тюменскаяобластьбезавтономныхокругов',\n",
    "    'тюменскаяобластьсао': 'тюменскаяобласть',\n",
    "    'хантымансийскийао': 'хантымансийскийавтономныйокругюгра',\n",
    "    'фо': 'федеральныйокруг',\n",
    "    'автокруг': 'автономныйокруг',\n",
    "    'автобласть': 'автономнаяобласть',\n",
    "    'автобл': 'автономнаяобласть',\n",
    "    'ао': 'автономныйокруг'}\n",
    "for key, value in replacements.items():\n",
    "    all_data_pdf.region = all_data_pdf.region.str.replace(key, value)           \n",
    "regions.loc[-1] = ['Читинская область', 76999999, 76999999, 'регион', 'читинскаяобласть']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76640e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем данные по регионам и индикаторам\n",
    "all_data = all_data_pdf.merge(regions, how='left', on='region').merge(indicators, how='left', on='indicator_short_code')\n",
    "all_data.index = all_data_pdf.index\n",
    "\n",
    "# Объединяем комментарии по нозологиям и индикаторам\n",
    "all_data['comment'] = all_data.apply(lambda row: \n",
    "                    row['comment_x'] if pd.notna(row['comment_x']) and pd.isna(row['comment_y']) else \n",
    "                    row['comment_y'] if pd.isna(row['comment_x']) and pd.notna(row['comment_y']) else \n",
    "                    f\"{row['comment_x']}; {row['comment_y']}\" if pd.notna(row['comment_x']) and pd.notna(row['comment_y']) else \n",
    "                    '', \n",
    "                    axis=1)\n",
    "\n",
    "# Наводим финальную красоту\n",
    "all_data = all_data.drop(columns={'region','indicator_short_code', 'comment_x', 'comment_y'})\n",
    "\n",
    "for col in ['indicator_section','indicator_name', 'indicator_unit', 'indicator_code',\n",
    "            'nosology_name','nosology_code','soc_dem_group',\n",
    "            'object_name', 'object_level', 'object_oktmo','object_okato','comment']:\n",
    "    all_data[col] = all_data[col].astype('string')  \n",
    "    \n",
    "all_data = all_data[['indicator_section','indicator_name', 'indicator_unit', 'indicator_code',\n",
    "                     'nosology_name','nosology_code','soc_dem_group',\n",
    "                     'object_name', 'object_level', 'object_oktmo','object_okato',\n",
    "                     'year','indicator_value','comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5c23914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выгружаем таблицу в csv\n",
    "all_data.to_csv('data/processed/cancer_care.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
